from llama_index.core.agent import FunctionCallingAgentWorker\n\nagent = FunctionCallingAgentWorker(\n    tools=tools,\n    llm=llm,\n    prefix_messages=prefix_messages,\n    max_function_calls=10,\n    allow_parallel_tool_calls=False,\n    verbose=True\n).as_agent()    verbose=True,  # Enable verbose output
).as_agent()